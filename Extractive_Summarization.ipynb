{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyEXF24iQx+Hmh4LxzrXW6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saish31/Python-Projects/blob/main/Extractive_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extractive Text Summarization"
      ],
      "metadata": {
        "id": "JATL4tnWO7UI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extractive Summarization**: The extractive approach involves picking up the most important phrases and lines from the documents.\n",
        "\n",
        "LexRank is a graph-based method for automatic text summarization that identifies the most important sentences in a document by analyzing their similarity to other sentences. It essentially treats sentences as nodes in a graph and calculates their importance based on how many other sentences recommend them. This recommendation is determined by sentence similarity, often measured using cosine similarity."
      ],
      "metadata": {
        "id": "DtxwAtC_O3iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "S1T3T1jIO6ac"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQs9-y19QVqv",
        "outputId": "cea42015-b764-4322-c77d-28fb9fb1a123"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtractiveSummarizer:\n",
        "    \"\"\"\n",
        "    High-level extractive summarizer using TextRank algorithm.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_sentences: int = 3):\n",
        "        self.num_sentences = num_sentences\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def _tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"Split text into sentences.\"\"\"\n",
        "        return nltk.sent_tokenize(text)\n",
        "\n",
        "    def _build_similarity_graph(self, sentences: list[str]) -> nx.Graph:\n",
        "        \"\"\"Create a graph where nodes are sentences and edges are TF-IDF cosine similarities.\"\"\"\n",
        "        tfidf_matrix = self.vectorizer.fit_transform(sentences)\n",
        "        similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
        "        # Zero out self-similarities\n",
        "        for i in range(len(similarity_matrix)):\n",
        "            similarity_matrix[i, i] = 0.0\n",
        "\n",
        "        graph = nx.from_numpy_array(similarity_matrix)\n",
        "        return graph\n",
        "\n",
        "    def summarize(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate an extractive summary by selecting top-ranked sentences.\n",
        "        :param text: Input document as a single string.\n",
        "        :return: Concise summary as a string.\n",
        "        \"\"\"\n",
        "        sentences = self._tokenize(text)\n",
        "        if len(sentences) <= self.num_sentences:\n",
        "            return text\n",
        "\n",
        "        graph = self._build_similarity_graph(sentences)\n",
        "        scores = nx.pagerank(graph)\n",
        "        # Rank sentences by score\n",
        "        ranked = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
        "        top_indices = [idx for idx, _ in ranked[: self.num_sentences]]\n",
        "        # Preserve original order\n",
        "        top_indices.sort()\n",
        "\n",
        "        summary = ' '.join(sentences[i] for i in top_indices)\n",
        "        return summary"
      ],
      "metadata": {
        "id": "ITWxnoCoPKyI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    sample = (\n",
        "        \"\"\"\n",
        "        Immediately after the verdict, in a statement released through her spokesperson, Amber had said she was ‘sad’ she had ‘lost the case’. The jury had also found Johnny guilty of defamation on one count and ordered him to pay Amber $2 million in damages. However, most legal experts said the case had been vindication for Johnny.\n",
        "        Speaking about it on Today Show, Amber said about the jury, “I don’t blame them. I actually understand. He’s a beloved character and people feel they know him. He’s a fantastic actor.”\n",
        "        The actor also addressed the memes that have been made about her and the hate coming her way on social media through the trial. She said, “I don’t care what one thinks about me or what judgments you want to make about what happened in the privacy of my own home, in my marriage, behind closed doors. I don’t presume the average person should know those things. And so I don’t take it personally. But even somebody who is sure I’m deserving of all this hate and vitriol, even if you think that I’m lying, you still couldn’t look me in the eye and tell me that you think on social media there’s been a fair representation. You cannot tell me that you think that this has been fair.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    summarizer = ExtractiveSummarizer(num_sentences=2)\n",
        "    print(\"Summary:\", summarizer.summarize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkVTno_cRwLP",
        "outputId": "fc2c243a-5723-4f37-8bf4-bef8c2bbd3fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Speaking about it on Today Show, Amber said about the jury, “I don’t blame them. But even somebody who is sure I’m deserving of all this hate and vitriol, even if you think that I’m lying, you still couldn’t look me in the eye and tell me that you think on social media there’s been a fair representation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RAT-p7RSpjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}